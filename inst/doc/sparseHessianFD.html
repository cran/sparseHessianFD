<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Michael Braun" />

<meta name="date" content="2015-01-28" />

<title>Using sparseHessianFD</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link href="data:text/css,body%20%7B%0A%20%20background%2Dcolor%3A%20%23fff%3B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20max%2Dwidth%3A%20700px%3B%0A%20%20overflow%3A%20visible%3B%0A%20%20padding%2Dleft%3A%202em%3B%0A%20%20padding%2Dright%3A%202em%3B%0A%20%20font%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0A%20%20font%2Dsize%3A%2014px%3B%0A%20%20line%2Dheight%3A%201%2E35%3B%0A%7D%0A%0A%23header%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0A%0A%23TOC%20%7B%0A%20%20clear%3A%20both%3B%0A%20%20margin%3A%200%200%2010px%2010px%3B%0A%20%20padding%3A%204px%3B%0A%20%20width%3A%20400px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20border%2Dradius%3A%205px%3B%0A%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20font%2Dsize%3A%2013px%3B%0A%20%20line%2Dheight%3A%201%2E3%3B%0A%7D%0A%20%20%23TOC%20%2Etoctitle%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%20%20font%2Dsize%3A%2015px%3B%0A%20%20%20%20margin%2Dleft%3A%205px%3B%0A%20%20%7D%0A%0A%20%20%23TOC%20ul%20%7B%0A%20%20%20%20padding%2Dleft%3A%2040px%3B%0A%20%20%20%20margin%2Dleft%3A%20%2D1%2E5em%3B%0A%20%20%20%20margin%2Dtop%3A%205px%3B%0A%20%20%20%20margin%2Dbottom%3A%205px%3B%0A%20%20%7D%0A%20%20%23TOC%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dleft%3A%20%2D2em%3B%0A%20%20%7D%0A%20%20%23TOC%20li%20%7B%0A%20%20%20%20line%2Dheight%3A%2016px%3B%0A%20%20%7D%0A%0Atable%20%7B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dcolor%3A%20%23DDDDDD%3B%0A%20%20border%2Dstyle%3A%20outset%3B%0A%20%20border%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0A%20%20border%2Dwidth%3A%202px%3B%0A%20%20padding%3A%205px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%20%20line%2Dheight%3A%2018px%3B%0A%20%20padding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0A%20%20border%2Dleft%2Dstyle%3A%20none%3B%0A%20%20border%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Ap%20%7B%0A%20%20margin%3A%200%2E5em%200%3B%0A%7D%0A%0Ablockquote%20%7B%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20padding%3A%200%2E25em%200%2E75em%3B%0A%7D%0A%0Ahr%20%7B%0A%20%20border%2Dstyle%3A%20solid%3B%0A%20%20border%3A%20none%3B%0A%20%20border%2Dtop%3A%201px%20solid%20%23777%3B%0A%20%20margin%3A%2028px%200%3B%0A%7D%0A%0Adl%20%7B%0A%20%20margin%2Dleft%3A%200%3B%0A%7D%0A%20%20dl%20dd%20%7B%0A%20%20%20%20margin%2Dbottom%3A%2013px%3B%0A%20%20%20%20margin%2Dleft%3A%2013px%3B%0A%20%20%7D%0A%20%20dl%20dt%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%7D%0A%0Aul%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%7D%0A%20%20ul%20li%20%7B%0A%20%20%20%20list%2Dstyle%3A%20circle%20outside%3B%0A%20%20%7D%0A%20%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dbottom%3A%200%3B%0A%20%20%7D%0A%0Apre%2C%20code%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20color%3A%20%23333%3B%0A%7D%0Apre%20%7B%0A%20%20white%2Dspace%3A%20pre%2Dwrap%3B%20%20%20%20%2F%2A%20Wrap%20long%20lines%20%2A%2F%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20margin%3A%205px%200px%2010px%200px%3B%0A%20%20padding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Acode%20%7B%0A%20%20font%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0A%20%20font%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0A%20%20padding%3A%202px%200px%3B%0A%7D%0A%0Adiv%2Efigure%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0A%20%20background%2Dcolor%3A%20%23FFFFFF%3B%0A%20%20padding%3A%202px%3B%0A%20%20border%3A%201px%20solid%20%23DDDDDD%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20margin%3A%200%205px%3B%0A%7D%0A%0Ah1%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%20%20font%2Dsize%3A%2035px%3B%0A%20%20line%2Dheight%3A%2040px%3B%0A%7D%0A%0Ah2%20%7B%0A%20%20border%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20padding%2Dbottom%3A%202px%3B%0A%20%20font%2Dsize%3A%20145%25%3B%0A%7D%0A%0Ah3%20%7B%0A%20%20border%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20font%2Dsize%3A%20120%25%3B%0A%7D%0A%0Ah4%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0A%20%20margin%2Dleft%3A%208px%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Ah5%2C%20h6%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23ccc%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Aa%20%7B%0A%20%20color%3A%20%230033dd%3B%0A%20%20text%2Ddecoration%3A%20none%3B%0A%7D%0A%20%20a%3Ahover%20%7B%0A%20%20%20%20color%3A%20%236666ff%3B%20%7D%0A%20%20a%3Avisited%20%7B%0A%20%20%20%20color%3A%20%23800080%3B%20%7D%0A%20%20a%3Avisited%3Ahover%20%7B%0A%20%20%20%20color%3A%20%23BB00BB%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%0A%2F%2A%20Class%20described%20in%20https%3A%2F%2Fbenjeffrey%2Ecom%2Fposts%2Fpandoc%2Dsyntax%2Dhighlighting%2Dcss%0A%20%20%20Colours%20from%20https%3A%2F%2Fgist%2Egithub%2Ecom%2Frobsimmons%2F1172277%20%2A%2F%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Keyword%20%2A%2F%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%2F%2A%20DataType%20%2A%2F%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%2F%2A%20DecVal%20%28decimal%20values%29%20%2A%2F%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20BaseN%20%2A%2F%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Float%20%2A%2F%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Char%20%2A%2F%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20String%20%2A%2F%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%2F%2A%20Comment%20%2A%2F%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%2F%2A%20OtherToken%20%2A%2F%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20AlertToken%20%2A%2F%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Function%20calls%20%2A%2F%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%2F%2A%20ErrorTok%20%2A%2F%0A%0A" rel="stylesheet" type="text/css" />

</head>

<body>



<div id="header">
<h1 class="title">Using sparseHessianFD</h1>
<h4 class="author"><em>Michael Braun</em></h4>
<h4 class="date"><em>2015-01-28</em></h4>
</div>


<p>The <strong>sparseHessianFD</strong> package provides an interface to ACM TOMS Algorithm 636 for estimating a Hessian that is sparse, in that a large proportion of the cross-partial derivatives are zero. The user provides the following:</p>
<ol style="list-style-type: decimal">
<li>an R function that returns the value of the objective function whene valuated at a variable vector <span class="math">\(P\)</span>;</li>
<li>an R function that returns the gradient of that objective function at <span class="math">\(P\)</span>; and</li>
<li>the row and column indices for the non-zero elements in the lower triangle of the Hessian.</li>
</ol>
<p>The non-zero elements are estimated through finite differencing of the gradients in a way that exploits the sparsity pattern of the Hessian.</p>
<p>There are, of course, other methods to estimate sparse Hessians, such as automatic differentiation (AD). However, operator-overloading implementations of AD require the objective function to be written using specialized libraries. These libraries are available for C++, Fortran, Matlab, Python, and possibly other programming languages and environments. At the present, there are no AD implementations that are “native” to R. Other than deriving the Hessian analytically, finite differencing remains the best option when the objective and gradient functions are written purely in R.</p>
<div id="example-function" class="section level2">
<h2>Example function</h2>
<p>Before going into the details of how to use the package, let’s consider the following example of an objective function with a sparse Hessian. Suppose we have a dataset of <span class="math">\(N\)</span> households, each with <span class="math">\(T\)</span> opportunities to purchase a particular product. Let <span class="math">\(y_i\)</span> be the number of times household <span class="math">\(i\)</span> purchases the product, out of the <span class="math">\(T\)</span> purchase opportunities. Furthermore, let <span class="math">\(p_i\)</span> be the probability of purchase; <span class="math">\(p_i\)</span> is the same for all <span class="math">\(T\)</span> opportunities, so we can treat <span class="math">\(y_i\)</span> as a binomial random variable. The purchase probability <span class="math">\(p_i\)</span> is heterogeneous, and depends on both <span class="math">\(k\)</span> continuous covariates <span class="math">\(x_i\)</span>, and a heterogeneous coefficient vector <span class="math">\(\beta_i\)</span>, such that <span class="math">\[
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
\]</span></p>
<p>The coefficients can be thought of as sensitivities to the covariates, and they are distributed across the population of households following a multivariate normal distribution with mean <span class="math">\(\mu\)</span> and covariance <span class="math">\(\Sigma\)</span>. We assume that we know <span class="math">\(\Sigma\)</span>, but we do not know <span class="math">\(\mu\)</span>. Instead, we place a multivariate normal prior on <span class="math">\(\mu\)</span>, with mean <span class="math">\(0\)</span> and covariance <span class="math">\(\Omega_0\)</span>. Thus, each <span class="math">\(\beta_i\)</span>, and <span class="math">\(\mu\)</span> are <span class="math">\(k-\)</span>dimensional vectors, and the total number of unknown variables in the model is <span class="math">\((N+1)k\)</span>.</p>
<p>The log posterior density, ignoring any normalization constants, is <span class="math">\[
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
\]</span></p>
<p>Since the <span class="math">\(\beta_i\)</span> are drawn iid from a multivariate normal, <span class="math">\(\dfrac{\partial^2\log\pi }{\partial\beta_i\beta_j}=0\)</span> for all <span class="math">\(i\neq j\)</span>. We also know that all of the <span class="math">\(\beta_i\)</span> are correlated with <span class="math">\(\mu\)</span>. The structure of the Hessian depends on how the variables are ordered within the vector. One such ordering is to group all of the coefficients for each unit together.</p>
<p><span class="math">\[
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
\]</span></p>
<p>In this case, the Hessian has a “block-arrow” structure. For example, if <span class="math">\(N=6\)</span> and <span class="math">\(k=2\)</span>, then there are 14 total variables, and the Hessian will have the following pattern.</p>
<pre><code>## 14 x 14 sparse Matrix of class &quot;lgCMatrix&quot;
##                                  
##  [1,] | | . . . . . . . . . . | |
##  [2,] | | . . . . . . . . . . | |
##  [3,] . . | | . . . . . . . . | |
##  [4,] . . | | . . . . . . . . | |
##  [5,] . . . . | | . . . . . . | |
##  [6,] . . . . | | . . . . . . | |
##  [7,] . . . . . . | | . . . . | |
##  [8,] . . . . . . | | . . . . | |
##  [9,] . . . . . . . . | | . . | |
## [10,] . . . . . . . . | | . . | |
## [11,] . . . . . . . . . . | | | |
## [12,] . . . . . . . . . . | | | |
## [13,] | | | | | | | | | | | | | |
## [14,] | | | | | | | | | | | | | |</code></pre>
<p>There are 196 elements in this symmetric matrix, but only 76 are non-zero, and only 45 values are unique. Although the reduction in RAM from using a sparse matrix structure for the Hessian may be modest, consider what would happen if <span class="math">\(N=1000\)</span> instead. In that case, there are 2002 variables in the problem, and more than <span class="math">\(4\)</span> million elements in the Hessian. However, only <span class="math">\(12004\)</span> of those elements are non-zero. If we work with only the lower triangle of the Hessian we only need to work with only 7003 values.</p>
<p>Another possibility is to group coefficients for each covariate together.</p>
<p><span class="math">\[
\beta_{11},...,\beta_{1N},\beta_{21},...,\beta_{2N},...,...,\beta_{k1},...,\beta_{kN},\mu_1,...,\mu_k
\]</span></p>
<p>Now the Hessian has an “off-diagonal” sparsity pattern.</p>
<pre><code>## 14 x 14 sparse Matrix of class &quot;lgCMatrix&quot;
##                                  
##  [1,] | . . . . . | . . . . . | |
##  [2,] . | . . . . . | . . . . | |
##  [3,] . . | . . . . . | . . . | |
##  [4,] . . . | . . . . . | . . | |
##  [5,] . . . . | . . . . . | . | |
##  [6,] . . . . . | . . . . . | | |
##  [7,] | . . . . . | . . . . . | |
##  [8,] . | . . . . . | . . . . | |
##  [9,] . . | . . . . . | . . . | |
## [10,] . . . | . . . . . | . . | |
## [11,] . . . . | . . . . . | . | |
## [12,] . . . . . | . . . . . | | |
## [13,] | | | | | | | | | | | | | |
## [14,] | | | | | | | | | | | | | |</code></pre>
<p>In both cases, the number of non-zeros is the same. The block-diagonal case may lead to slightly faster initialization, but repeated computation of the Hessian will take the same time as with the “off-diagonal” case.</p>
</div>
<div id="using-the-package" class="section level2">
<h2>Using the package</h2>
<p>The functions for computing the objective function, gradient and Hessian for this example are in the R/binary.R file. The package also includes a sample dataset with simulated data from the binary choice example.</p>
<p>To start, we load the data, set some dimension parameters, set prior values for <span class="math">\(\Sigma^{-1}\)</span> and <span class="math">\(\Omega^{-1}\)</span>, and simulate a vector of variables at which to evaluate the function.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
<span class="kw">data</span>(binary)
<span class="kw">str</span>(binary)
## List of 3
##  $ Y: int [1:30] 3 11 11 9 5 9 19 19 10 14 ...
##  $ X: num [1:2, 1:30] 1.5587 0.0705 0.1293 1.7151 0.4609 ...
##  $ T: num 20
N &lt;-<span class="st"> </span><span class="kw">length</span>(binary$Y)
k &lt;-<span class="st"> </span><span class="kw">NROW</span>(binary$X)
nvars &lt;-<span class="st"> </span><span class="kw">as.integer</span>(N*k +<span class="st"> </span>k)
P &lt;-<span class="st"> </span><span class="kw">rnorm</span>(nvars) ## random starting values
priors &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">inv.Sigma =</span> <span class="kw">rWishart</span>(<span class="dv">1</span>,k<span class="dv">+5</span>,<span class="kw">diag</span>(k))[,,<span class="dv">1</span>],
               <span class="dt">inv.Omega =</span> <span class="kw">diag</span>(k))</code></pre>
<p>This dataset represents the simulated choices for <span class="math">\(N= 30\)</span> customers over <span class="math">\(T= TRUE\)</span> purchase opportunties, where the probability of purchase is influenced by <span class="math">\(k= 2\)</span> covariates.</p>
<p>The objective function for the binary choice example is <code>binary.f</code> and the gradient function is <code>binary.grad</code>. The first argument to both is the variable vector, and the argument lists must be the same for both. For this example, we need to provide the data list (<span class="math">\(X\)</span>, <span class="math">\(Y\)</span> and <span class="math">\(T\)</span>) and the prior parameter list (<span class="math">\(\Sigma^{-1}\)</span> and <span class="math">\(\Omega^{-1}\)</span>). The functions also have an <code>order.row</code> argument to change the ordering of the variables (and thus, the sparsity pattern). If <code>order.row=TRUE</code>, then the Hessian will have an off-diagonal pattern. If <code>order.row=FALSE</code>, then the Hessian will have a block-arrow pattern.</p>
<p>For testing and demonstration purposes, we also have a <code>binary.hess</code> function that returns the Hessian as a sparse <code>dgCMatrix</code> object (see the Matrix package).</p>
<pre class="sourceCode r"><code class="sourceCode r">true.f &lt;-<span class="st"> </span><span class="kw">binary.f</span>(P, binary, priors, <span class="dt">order.row =</span> <span class="ot">FALSE</span>)
true.grad &lt;-<span class="st"> </span><span class="kw">binary.grad</span>(P, binary, priors, <span class="dt">order.row =</span> <span class="ot">FALSE</span>)
true.hess &lt;-<span class="st"> </span><span class="kw">binary.hess</span>(P, binary, priors, <span class="dt">order.row =</span> <span class="ot">FALSE</span>)</code></pre>
<p>The sparsity pattern of the Hessian is specified by two <strong>integer</strong> vectors: one each for the row and column indices of the non-zero elements of the <strong><em>lower triangular part</em></strong> of the Hessian. If you happen have have an example of a matrix with the same sparsity pattern of the Hessian you are trying to compute, you can use the following convenience function to extract the appropriate index vectors.</p>
<pre class="sourceCode r"><code class="sourceCode r">pattern &lt;-<span class="st"> </span><span class="kw">Matrix.to.Coord</span>(<span class="kw">tril</span>(true.hess))
<span class="kw">str</span>(pattern)                
## List of 2
##  $ rows: int [1:213] 1 2 61 62 2 61 62 3 4 61 ...
##  $ cols: int [1:213] 1 1 1 1 2 2 2 3 3 3 ...</code></pre>
<p>Next, we create a new instance of a sparseHessianFD object with an “initial variable” <span class="math">\(P\)</span>, and the row and column indices of the non-zero elements in the lower triangle of the Hessian. We also pass in any other arguments for <code>binary.f</code> and <code>binary.grad</code>. We accept the default values for other arguments to <code>sparseHessianFD.new</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">obj &lt;-<span class="st"> </span><span class="kw">sparseHessianFD.new</span>(P, binary.f, binary.grad,
       <span class="dt">rows=</span>pattern$rows, <span class="dt">cols=</span>pattern$cols,
       <span class="dt">data=</span>binary, <span class="dt">priors=</span>priors,
       <span class="dt">order.row=</span><span class="ot">FALSE</span>)</code></pre>
<p>Now we can evaluate the function value, gradient and Hessian through <code>obj</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">f &lt;-<span class="st"> </span>obj$<span class="kw">fn</span>(P)
gr &lt;-<span class="st"> </span>obj$<span class="kw">gr</span>(P)
hs &lt;-<span class="st"> </span>obj$<span class="kw">hessian</span>(P)</code></pre>
<p>Note that the member functions in the sparseHessianFD class take only one argument: the variable vector. All of the other arguments are stored in <code>obj</code>.</p>
<p>Do we get the same results that we would get after calling <code>binary.f</code>, <code>binary.grad</code> and <code>binary.hess</code> directly? Let’s see.</p>
<pre class="sourceCode r"><code class="sourceCode r">

<span class="kw">all.equal</span>(f, true.f)
## [1] TRUE
<span class="kw">all.equal</span>(gr, true.grad)
## [1] TRUE
<span class="kw">all.equal</span>(hs, true.hess)                         
## [1] TRUE</code></pre>
<p>If there is any difference, keep in mind that <code>hs</code> is a numerical estimate that is not always exact. I certainly wouldn’t worry about mean relative differences smaller than, say, <span class="math">\(10^{-6}\)</span>.</p>
<div id="speed-comparison" class="section level3">
<h3>Speed comparison</h3>
<p>Instead of using this package, we could treat the Hessian as dense, and use the hessian function numDeriv package. The advantage of the numDeriv package is that it does not require the gradient. However, you can see that it takes some time to run.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(numDeriv)
hess.time &lt;-<span class="st"> </span><span class="kw">system.time</span>(H1 &lt;-<span class="st"> </span>obj$<span class="kw">hessian</span>(P))
fd.time &lt;-<span class="st"> </span><span class="kw">system.time</span>(H2 &lt;-<span class="st"> </span><span class="kw">hessian</span>(obj$fn, P))
H2 &lt;-<span class="st"> </span><span class="kw">drop0</span>(H2, <span class="fl">1e-07</span>)  ## treat values &lt; 1e-8 as zero
<span class="kw">print</span>(hess.time)</code></pre>
<pre><code>##    user  system elapsed 
##   0.002   0.000   0.002</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fd.time)</code></pre>
<pre><code>##    user  system elapsed 
##   3.388   0.022   3.411</code></pre>
<p>The sparseHessianFD package can be substantially faster than estimating a dense Hessian by brute force finite differencing. The cost of this speed up is that the user does need to provide the gradient and the sparsity structure. As with everything in life, there are trade-offs.</p>
</div>
</div>
<div id="quick-summary" class="section level2">
<h2>Quick summary</h2>
<p>In short, to use the package, follow the following steps:</p>
<ol style="list-style-type: decimal">
<li>Write R functions to return the value and gradient of the objective function.</li>
<li>Determine the row and column indices of the non-zero elements of the lower triangle of the Hessian.</li>
<li>Pick a variable vector (i.e., a starting value) at which you can initialize the sparseHessian object. It doesn’t really matter what this vector is, as long as the function value and gradient elements are all finite.</li>
<li>Create a new sparseHessianFD object using the sparseHessianFD.new function. For this example, call that object F.</li>
<li>Compute Hessian at x by calling F$hessian(x).</li>
<li>???</li>
<li>Profit.</li>
</ol>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
